{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19ae8c1",
   "metadata": {},
   "source": [
    "# 6th Tutorial - Attention in PyTorch\n",
    "\n",
    "In this tutorial, you will implement the attention mechanism of transformers as described in [Attention is all you need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) on your own.\n",
    "\n",
    "Source: https://d2l.ai/chapter_attention-mechanisms-and-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6e2e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701c3a6",
   "metadata": {},
   "source": [
    "First, implement the single-head QKV attention mechanism. This does not include learnable parameters so far. \n",
    "You can use the `masked_softmax` utility below to account for inputs of differents that have been padded.\n",
    "Use batch matrix multiplication for efficiency and apply random dropout to the attention scores.\n",
    "\n",
    "Also note that we need to keep the order of magnitude of the arguments in the exponential function under control. Assume that all the elements of the query $\\mathbf{q} \\in \\mathbb{R}^d$ and the key $\\mathbf{k}_i \\in \\mathbb{R}^d$ are independent and identically drawn random variables with zero mean and unit variance. The dot product between both vectors has zero mean and a variance of $d$. To ensure that the variance of the dot product still remains $1$ regardless of vector length, we use the *scaled dot product attention* scoring function. That is, we rescale the dot product by $1/\\sqrt{d}$. We thus arrive at the first commonly used attention function that is used, e.g., in Transformers:\n",
    "\n",
    "$$ a(\\mathbf{q}, \\mathbf{k}_i) = \\mathbf{q}^\\top \\mathbf{k}_i / \\sqrt{d}.$$\n",
    "\n",
    "\n",
    "Note that attention weights $\\alpha$ still need normalizing. We can simplify this further by using the softmax operation:\n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(\\mathbf{q}^\\top \\mathbf{k}_i / \\sqrt{d})}{\\sum_{j=1} \\exp(\\mathbf{q}^\\top \\mathbf{k}_j / \\sqrt{d})}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ce124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):  #@save\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "    def _sequence_mask(X, valid_len, value=0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9925f",
   "metadata": {},
   "source": [
    "Test your implementation with the following code. Is the shape of the output correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.normal(0, 1, (2, 3, 2))\n",
    "keys = torch.normal(0, 1, (2, 10, 2))\n",
    "values = torch.normal(0, 1, (2, 10, 4))\n",
    "\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "\n",
    "output = attention(queries, keys, values)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82a0f7",
   "metadata": {},
   "source": [
    "Now, implement the multihead attention. Here, we use learnable, linear mappings to project the input queries, keys, and values to queries, keys, and values for different heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e79d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        \n",
    "    def transpose_qkv(self, X):\n",
    "        \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "        # TODO\n",
    "    \n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # Shape of queries, keys, or values:\n",
    "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "        # After transposing, shape of output queries, keys, or values:\n",
    "        # (batch_size * num_heads, no. of queries or key-value pairs,\n",
    "        # num_hiddens / num_heads)\n",
    "        \n",
    "        # TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e0373",
   "metadata": {},
   "source": [
    "Test your implementation with the following code. Is the shape of the output correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e07960",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "X = torch.normal(0, 1, (batch_size, num_queries, num_hiddens))\n",
    "Y = torch.normal(0, 1, (batch_size, num_kvpairs, num_hiddens))\n",
    "output = attention(X, Y, Y, valid_lens)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66431cd9",
   "metadata": {},
   "source": [
    "How can we get self-attention with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371eeaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f271c61",
   "metadata": {},
   "source": [
    "What are positional encodings? Why do we need them? What two kinds of positional encodings exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771877da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
