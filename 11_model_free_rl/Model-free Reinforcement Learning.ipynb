{
 "cells": [
  {
   "attachments": {
    "grafik.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-free Reinforcement Learning\n",
    "\n",
    "Assume that we have the same grid world as in the previous tutorial:\n",
    "\n",
    "![grafik.png](attachment:grafik.png)\n",
    "\n",
    "The goal of the agent whose position is at the bottom left corner is to maximize the expected future reward.\n",
    "The top right (reward +1) and the middle right cell (reward -1) are terminal states.\n",
    "There are four different actions the agent can choose, namely go up, down, left and right.\n",
    "However, there is a chance that the agent will result in a wrong grid cell by accident.\n",
    "If the action is going up, there is a chance of 0.8 that the agent moves up, a chance of 0.1 that he moves left and a chance of 0.1 that he moves right.\n",
    "All other actions behave accordingly.\n",
    "If the agent would bounce on a wall it stays at the current position.\n",
    "Note that the agent cannot move through the grayed-out grid cell.\n",
    "Assume a gamma value of $\\gamma=0.9$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we define the described environment as an MDP.  The start (bottom left) is defined as index 0. The cell indices then increase from left to right and continue in the next row above. I.e. the cell above the start has index 4, the one with -1 reward has index 7, the cell with +1 index 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"  \n",
    "    Rows in _transitions correspond to actions: 0: up, 1: right, 2: left, 3: down\n",
    "    - First value in brackets is the index change in the environment grid -1:'left'| +1:'up'|-4:'down'|+4:'up', \n",
    "    - Second value is the probability of going into that direction when choosing an action\n",
    "    \"\"\"\n",
    "    _transitions = np.array([    \n",
    "        [[4, 0.8], [-1, 0.1], [1, 0.1]],  #Action 0 (up) -> with prob 0.8 go up, with 0.1 go left or right\n",
    "        [[1, 0.8], [-4, 0.1], [4, 0.1]],  #with prob 0.8 go right\n",
    "        [[-1, 0.8], [-4, 0.1], [4, 0.1]], #with prob 0.8 go left\n",
    "        [[-4, 0.8], [-1, 0.1], [1, 0.1]]  #with prob 0.8 go down\n",
    "        ])\n",
    "    \n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.reward = np.zeros(3*4)\n",
    "        self.reward[-1] = +1  #index -1 = 11\n",
    "        #self.reward[5] = -1\n",
    "        self.reward[7] = -1\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def rewards(self, state):\n",
    "        \"\"\"\n",
    "        Returns the reward for the given state as scalar.\n",
    "        \"\"\"\n",
    "        return self.reward[state]\n",
    "    \n",
    "    def _bounces(self, state, s):\n",
    "        return np.sum(\n",
    "            np.abs(np.array(np.where(np.arange(12).reshape((3,4)) == state)) \n",
    "                   - np.array(np.where(np.arange(12).reshape((3,4)) == s)))\n",
    "        ) > 1\n",
    "    \n",
    "    def transition_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        For a given state action pair, returns a list of follow states including their probabilities.\n",
    "        If a state has no follow states, we return [0, 0].\n",
    "        Example:\n",
    "        Suppose (s, a) =  (4, 'up'). We go up to 8 with probability 0.8, left or right with probability 0.1. \n",
    "        In the latter two cases, we bump into the wall and stay at 4 Thus the output is [[8, 0.8], [4, 0.1], [4, 0.1]]\n",
    "        \"\"\"\n",
    "        if state == 7 or state == 11:\n",
    "            return np.zeros((0, 2))\n",
    "        trans = MDP._transitions[action].copy()\n",
    "        trans = trans.T\n",
    "        trans[0,:] += state \n",
    "        trans = trans.T     #follow states\n",
    "        \n",
    "        #if agent bounces into wall, stay at current state\n",
    "        for i, (s_new, prob) in enumerate(trans):\n",
    "            if s_new < 0 or s_new >= 3*4 or s_new == 5 or self._bounces(state, s_new):\n",
    "                trans[i,0] = state\n",
    "        return trans\n",
    "    \n",
    "mdp = MDP()\n",
    "#mdp.transition_probs(4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_environment(values):\n",
    "    plt.figure()\n",
    "    reshaped = values.reshape((3,4))\n",
    "    ax = sns.heatmap(reshaped, annot=True)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylim(-0.1, 3.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free\n",
    "\n",
    "Since we use model-free reinforcement learning, we do not know the model (the underlying MDP). Hence, the agent can only learn from experience (observations and rewards) within the environment. In the following cell we define the environment the agent interacts with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment:\n",
    "    def __init__(self):\n",
    "        self.mdp = MDP()\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "    \n",
    "    def act(self, action):\n",
    "        trans = self.mdp.transition_probs(self.state, action)\n",
    "        if len(trans) > 0:\n",
    "            self.state = int(np.random.choice(trans[:,0], p=trans[:,1]))\n",
    "        else:\n",
    "            raise Exception(\"terminal state has no action!\")\n",
    "        reward = self.mdp.rewards(self.state)\n",
    "        done = self.state == 7 or self.state == 11\n",
    "        return self.state, reward, done\n",
    "    \n",
    "    def actions(self):\n",
    "        return range(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Complete the Q-Learning algorithm in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, alpha, gamma, epsilon, epochs):\n",
    "    \"\"\"\n",
    "    Chooses an action a in state (observation) s epsilon-greedily and updates the q-values Q(s, a) by:\n",
    "    Q(s, a) = Q(s, a) + alpha*(R + gamma * max_a'(Q(s', a')) - Q(s, a))\n",
    "    \n",
    "    alpha: learning rate\n",
    "    gamma: discount factor\n",
    "    epsilon: exploration rate\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    q_table = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        action = None\n",
    "        \n",
    "        while not done:\n",
    "            # TODO\n",
    "            raise Exception(\"Not implemented!\")\n",
    "        \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnvironment()\n",
    "q_table = q_learning(env, 0.01, 0.9, 0.5, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we do not have any action to choose in the terminal states and \n",
    "# thus we do not have any q-values for terminal states\n",
    "utilities = np.asarray([np.max(q_table.get(i, [np.nan])) for i in range(12)])\n",
    "print(\"Utility values (Q-learning)\")\n",
    "plot_environment(utilities)\n",
    "\n",
    "policy = np.asarray([np.nan if state not in q_table else np.argmax(q_table[state]) for state in range(len(utilities))])\n",
    "policy[5] = policy[7] = policy[11] = -1\n",
    "\n",
    "print(\"Policy (Q-learning)\")\n",
    "plot_environment(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "Complete the SARSA algorithm in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sarsa(env, alpha, gamma, epsilon, epochs):\n",
    "    \"\"\"\n",
    "    Chooses an action a in state s (epsilon-greedy) and updates the q-values Q(s, a) by:\n",
    "    Q(s, a) = Q(s, a) + alpha*(R + gamma * (Q(s', a')) - Q(s, a)) where a' is the next epsilon greedy acti\n",
    "    \n",
    "    alpha: learning rate\n",
    "    gamma: discount factor\n",
    "    epsilon: exploration rate\n",
    "    \"\"\"\n",
    "    \n",
    "    q_table = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        action = None\n",
    "        \n",
    "        while not done:\n",
    "            # TODO\n",
    "            raise Exception(\"Not implemented!\")\n",
    "        \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnvironment()\n",
    "\n",
    "q_table = sarsa(env, 0.01, 0.9, 0.5, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities = np.asarray([np.max(q_table.get(i, [np.nan])) for i in range(12)])\n",
    "print(\"Utility values (SARSA)\")\n",
    "plot_environment(utilities)\n",
    "\n",
    "policy = np.asarray([np.nan if state not in q_table else np.argmax(q_table[state]) for state in range(len(utilities))])\n",
    "policy[5] = policy[7] = policy[11] = -1\n",
    "\n",
    "print(\"Policy (SARSA)\")\n",
    "plot_environment(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Why is the policy from SARSA different? Is it optimal?\n",
    "What do we need to change such that the policies are equal?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
